import copy
import os
os.environ["PYOPENGL_PLATFORM"] = "OSMesa"
import sys
sys.path.append(os.path.abspath(os.path.dirname(__file__)))     # append current dir to PATH
sys.path.append(os.path.join(os.path.abspath(os.path.dirname(__file__)), "../"))

import torch
from tqdm import tqdm
import cv2
import time
import torchvision.transforms as standard
import numpy as np

from base import Tester
from config import cfg
from utils.visualize import draw_2d_skeleton
from data.processing import inference_extraHM, augmentation, cv2pil, augmentation_real
import mediapipe as mp


mp_drawing = mp.solutions.drawing_utils
mp_drawing_styles = mp.solutions.drawing_styles
mp_hands = mp.solutions.hands


def rgb2gray(rgb):
    r, g, b = rgb[:,:,0], rgb[:,:,1], rgb[:,:,2]
    gray = 0.2989 * r + 0.5870 * g + 0.1140 * b
    return gray

def encode_hand_data(hand_result):
    """ Encode hand data to json format """

    """ Example """
    """
    handDataPackage['joints_0']
    handDataPackage['joints_1']
    if the hand is not detected, returns zero value joints 

    currently consider single hand
    """
    handDataPackage = dict()
    joints = list()
    num_joints = 21

    for joint_uvd in hand_result:
        for id in range(num_joints):
            joint = dict()
            joint['id'] = int(id)
            joint['u'] = float(joint_uvd[id, 0])
            joint['v'] = float(joint_uvd[id, 1])
            joint['d'] = float(joint_uvd[id, 2])
            joints.append(joint)
        break
    handDataPackage['joints'] = joints

    return handDataPackage


class HandTracker():
    def __init__(self):
        self.tester = Tester()
        self.tester._make_model()
        # self.detector = HandDetector()

        mean_std = ([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
        self.transform = standard.Compose([standard.ToTensor(), standard.Normalize(*mean_std)])

        if cfg.extra:
            self.extra_uvd_left = np.zeros((21, 3), dtype=np.float32)
            self.extra_uvd_right = np.zeros((21, 3), dtype=np.float32)
            self.idx = 0


        # self.prev_bbox_list = []
        # self.prev_flag_flip_list = []
        self.mediahand = mp_hands.Hands(static_image_mode=True, max_num_hands=1, min_detection_confidence=0.3)

        # do first iteration
        emptyImg = np.ones((256, 256), dtype=float)
        img_pil = cv2pil(emptyImg)
        img = self.transform(img_pil)
        img = torch.unsqueeze(img, 0).type(torch.float32)
        inputs = {'img': img}

        self.extra_uvd = np.zeros((21, 3), dtype=np.float32)
        extra_hm = inference_extraHM(self.extra_uvd, self.idx, reinit_num=10)
        inputs['extra'] = torch.unsqueeze(torch.from_numpy(extra_hm), dim=0)
        _ = self.tester.model(inputs)
        print("success on first run")

        self.img_w = 640
        self.img_h = 360
        self.crop_size = 360
        self.default_bbox = [240, 0, self.crop_size, self.crop_size]

        self.prev_coord = None

        self.winname = "test"
        cv2.namedWindow(self.winname)
        cv2.moveWindow(self.winname, 1920, 30)


    def Process_single_nomp(self, img): # input : img_cv
        t0 = time.time()
        if img.shape[-1] == 4:
            img = img[:, :, :-1]
        imgSize = (img.shape[0], img.shape[1])  # (360, 640)

        # image from hololens2 is fliped both direction
        # img = cv2.flip(img, 0)
        img_cv = np.copy(img)

        ## new hand detection. need to add re-initialization
        if self.prev_coord is not None:
            bbox = self.calc_bounding_rect_coords(self.img_w, self.img_h, self.prev_coord)
        else:
            bbox = self.default_bbox


        img_crop, img2bb_trans, bb2img_trans, _, _, = augmentation_real(img, bbox, flip=False)
        # cv2.imshow('img_crop', img_crop/255.0)
        # cv2.waitKey(1)


        # transform img
        img_pil = cv2pil(img_crop)
        img = self.transform(img_pil)
        img = torch.unsqueeze(img, 0).type(torch.float32)
        inputs = {'img': img}


        if cfg.extra:
            self.extra_uvd = np.copy(self.extra_uvd_right)

            # affine transform x,y coordinates with current crop info
            uv1 = np.concatenate((self.extra_uvd[:, :2], np.ones_like(self.extra_uvd[:, :1])), 1)
            self.extra_uvd[:, :2] = np.dot(img2bb_trans, uv1.transpose(1, 0)).transpose(1, 0)[:, :2]

            # normalize uv, depth is already relative value
            self.extra_uvd[:, :2] = self.extra_uvd[:, :2] / (cfg.input_img_shape[0] // 2) - 1

            extra_hm = inference_extraHM(self.extra_uvd, self.idx, reinit_num=10)
            inputs['extra'] = torch.unsqueeze(torch.from_numpy(extra_hm), dim=0)
            self.idx += 1

        t1 = time.time()
        with torch.no_grad():
            outs = self.tester.model(inputs).detach()
        t2 = time.time()

        outs = outs.to("cpu", non_blocking=True)
        coords_uvd = outs.numpy()[0]

        # normalized value to uv(pixel) range
        coords_uvd[:, :2] = (coords_uvd[:, :2] + 1) * (cfg.input_img_shape[0] // 2)

        # back to original image
        uv1 = np.concatenate((coords_uvd[:, :2], np.ones_like(coords_uvd[:, :1])), 1)
        coords_uvd[:, :2] = np.dot(bb2img_trans, uv1.transpose(1, 0)).transpose(1, 0)[:, :2]


        if cfg.extra:
            self.extra_uvd_right = np.copy(coords_uvd[cfg.num_vert:])

        # restore depth value after passing extra pose
        coords_uvd[:, 2] = coords_uvd[:, 2] * cfg.depth_box # + root_depth (we don't know here)

        # mesh_uvd = copy.deepcopy(all_uvd[:cfg.num_vert])  # (778, 3)
        coords_uvd = coords_uvd[cfg.num_vert:]   # (21, 3)

        t3 = time.time()
        # print("preprocess, inference, postprocess : ", t1-t0, t2-t1, t3-t2)

        ### visualize output in server ###
        # img_cv = draw_2d_skeleton(img_cv, coords_uvd)
        # cv2.imshow(self.winname, img_cv)
        # cv2.waitKey(1)

        return coords_uvd


    def Process_single(self, img): # input : img_cv
        t0 = time.time()
        if img.shape[-1] == 4:
            img = img[:, :, :-1]
        imgSize = (img.shape[0], img.shape[1])  # (360, 640)

        # image from hololens2 is fliped both direction
        img = cv2.flip(img, 0)
        img_cv = np.copy(img)

        ### hand detection with mediapipe (17ms)
        # currently extracting only right-side hand
        bbox_list, img_crop_list, img2bb_trans_list, bb2img_trans_list, flag_flip_list = self.extract_singlehand(img, imgSize)




        t1 = time.time()
        joint_uvd_list = []
        # mesh_uvd_list = []

        if len(bbox_list) == 0:
            print("no bbox, return zero joint")
            joint_uvd = np.zeros((21, 3), dtype=np.float32)
            joint_uvd_list.append(joint_uvd)
            return joint_uvd_list

        else:
            for bbox, img_crop, img2bb_trans, bb2img_trans, flag_flip in \
                    zip(bbox_list, img_crop_list, img2bb_trans_list, bb2img_trans_list, flag_flip_list):
                # transform img
                img_pil = cv2pil(img_crop)
                img = self.transform(img_pil)
                img = torch.unsqueeze(img, 0).type(torch.float32)
                inputs = {'img': img}

                if cfg.extra:
                    if flag_flip:
                        self.extra_uvd = np.copy(self.extra_uvd_left)
                    else:
                        self.extra_uvd = np.copy(self.extra_uvd_right)

                    # affine transform x,y coordinates with current crop info
                    uv1 = np.concatenate((self.extra_uvd[:, :2], np.ones_like(self.extra_uvd[:, :1])), 1)
                    self.extra_uvd[:, :2] = np.dot(img2bb_trans, uv1.transpose(1, 0)).transpose(1, 0)[:, :2]

                    # normalize uv, depth is already relative value
                    self.extra_uvd[:, :2] = self.extra_uvd[:, :2] / (cfg.input_img_shape[0] // 2) - 1

                    extra_hm = inference_extraHM(self.extra_uvd, self.idx, reinit_num=10)
                    inputs['extra'] = torch.unsqueeze(torch.from_numpy(extra_hm), dim=0)
                    self.idx += 1

                t2 = time.time()
                with torch.no_grad():
                    outs = self.tester.model(inputs).detach()
                t3 = time.time()

                outs = outs.to("cpu", non_blocking=True)
                coords_uvd = outs.numpy()[0]


                # normalized value to uv(pixel) range
                coords_uvd[:, :2] = (coords_uvd[:, :2] + 1) * (cfg.input_img_shape[0] // 2)

                # back to original image
                uv1 = np.concatenate((coords_uvd[:, :2], np.ones_like(coords_uvd[:, :1])), 1)
                coords_uvd[:, :2] = np.dot(bb2img_trans, uv1.transpose(1, 0)).transpose(1, 0)[:, :2]
                t5 = time.time()

                if cfg.extra:
                    if flag_flip:
                        self.extra_uvd_left = np.copy(coords_uvd[cfg.num_vert:])
                    else:
                        self.extra_uvd_right = np.copy(coords_uvd[cfg.num_vert:])

                # restore depth value after passing extra pose
                coords_uvd[:, 2] = coords_uvd[:, 2] * cfg.depth_box # + root_depth (we don't know)

                # mesh_uvd = copy.deepcopy(all_uvd[:cfg.num_vert])  # (778, 3)
                coords_uvd = coords_uvd[cfg.num_vert:]   # (21, 3)
                if flag_flip:
                    coords_uvd[:, 0] = imgSize[1] - coords_uvd[:, 0]
                    # mesh_uvd[:, 0] = imgSize[1] - mesh_uvd[:, 0]

                joint_uvd_list.append(coords_uvd)
                # mesh_uvd_list.append(mesh_uvd)

                t4 = time.time()
                print("detect, preprocess, inference, postprocess : ", t1-t0, t2-t1, t3-t2, t4-t3)

            ### visualize output in server ###
            # for joint_uvd in joint_uvd_list:
            #     img_cv = draw_2d_skeleton(img_cv, joint_uvd)
            # cv2.imshow('img_cv', img_cv)
            # cv2.waitKey(1)

            return joint_uvd_list


    def Process(self, img): # input : img_cv
        t0 = time.time()
        if img.shape[-1] == 4:
            img = img[:, :, :-1]
        imgSize = (img.shape[0], img.shape[1])  # (360, 640)

        img_cv = np.copy(img)
        ### hand detection with mediapipe (17ms)
        ## currently extracting only right-side hand
        bbox_list, img_crop_list, img2bb_trans_list, bb2img_trans_list, flag_flip_list = self.extract_singlehand(img)

        t1 = time.time()
        joint_uvd_list = []
        # mesh_uvd_list = []
        if len(bbox_list) == 0:
            print("no bbox, return zero joint")
            joint_uvd = np.zeros((21, 3), dtype=np.float32)
            joint_uvd_list.append(joint_uvd)
            return joint_uvd_list

        else:
            for bbox, img_crop, img2bb_trans, bb2img_trans, flag_flip in \
                    zip(bbox_list, img_crop_list, img2bb_trans_list, bb2img_trans_list, flag_flip_list):
                # crop_name = 'crop_{}'.format(debug_i)
                # debug_i += 1
                # cv2.imshow(crop_name, img_crop/255.)
                # cv2.waitKey(1)


                # transform img
                img_pil = cv2pil(img_crop)
                img = self.transform(img_pil)
                img = torch.unsqueeze(img, 0).type(torch.float32)
                inputs = {'img': img}

                if cfg.extra:
                    if flag_flip:
                        self.extra_uvd = np.copy(self.extra_uvd_left)
                    else:
                        self.extra_uvd = np.copy(self.extra_uvd_right)

                    # affine transform x,y coordinates with current crop info
                    uv1 = np.concatenate((self.extra_uvd[:, :2], np.ones_like(self.extra_uvd[:, :1])), 1)
                    self.extra_uvd[:, :2] = np.dot(img2bb_trans, uv1.transpose(1, 0)).transpose(1, 0)[:, :2]

                    # normalize uv, depth is already relative value
                    self.extra_uvd[:, :2] = self.extra_uvd[:, :2] / (cfg.input_img_shape[0] // 2) - 1

                    extra_hm = inference_extraHM(self.extra_uvd, self.idx, reinit_num=10)
                    inputs['extra'] = torch.unsqueeze(torch.from_numpy(extra_hm), dim=0)
                    self.idx += 1
                t2 = time.time()
                with torch.no_grad():
                    outs = self.tester.model(inputs)

                t3 = time.time()
                outs = {k: v.cpu().numpy() for k, v in outs.items()}
                coords_uvd = outs['coords'][0]

                # normalized value to uv(pixel) range
                coords_uvd[:, :2] = (coords_uvd[:, :2] + 1) * (cfg.input_img_shape[0] // 2)

                # back to original image
                uv1 = np.concatenate((coords_uvd[:, :2], np.ones_like(coords_uvd[:, :1])), 1)
                coords_uvd[:, :2] = np.dot(bb2img_trans, uv1.transpose(1, 0)).transpose(1, 0)[:, :2]

                if cfg.extra:
                    if flag_flip:
                        self.extra_uvd_left = np.copy(coords_uvd[cfg.num_vert:])
                    else:
                        self.extra_uvd_right = np.copy(coords_uvd[cfg.num_vert:])

                # restore depth value after passing extra pose
                coords_uvd[:, 2] = coords_uvd[:, 2] * cfg.depth_box # + root_depth (we don't know)

                # mesh_uvd = copy.deepcopy(all_uvd[:cfg.num_vert])  # (778, 3)
                joint_uvd = coords_uvd[cfg.num_vert:]   # (21, 3)
                if flag_flip:
                    joint_uvd[:, 0] = imgSize[1] - joint_uvd[:, 0]
                    # mesh_uvd[:, 0] = imgSize[1] - mesh_uvd[:, 0]

                joint_uvd_list.append(joint_uvd)
                # mesh_uvd_list.append(mesh_uvd)

                t4 = time.time()

            ### visualize output in server ###
            img_joint = np.copy(img_cv)
            for joint_uvd in joint_uvd_list:
                img_joint = draw_2d_skeleton(img_joint, joint_uvd)
            cv2.imshow('img_cv', img_joint)
            cv2.waitKey(1)

            print("detect, preprocess, inference, postprocess : ", t1 - t0, t2 - t1, t3-t2, t4-t3)

            return joint_uvd_list

    def extract_singlehand(self, img, imgSize):
        img_height = imgSize[0]
        img_width = imgSize[1]

        image = cv2.flip(img, 1)
        results = self.mediahand.process(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))

        if results.multi_hand_landmarks is not None:
            for hand_landmarks in results.multi_hand_landmarks:
                # Bounding box calculation
                bbox = self.calc_bounding_rect(img_width, img_height, hand_landmarks)

                img_crop_list, img2bb_trans_list, bb2img_trans_list = [], [], []
                img_crop, img2bb_trans, bb2img_trans, _, _, = augmentation_real(img, bbox, flip=False)

                # cv2.imshow('img_crop', img_crop/255.0)
                # cv2.waitKey(1)

                bbox_list = [bbox]
                img_crop_list.append(img_crop)
                img2bb_trans_list.append(img2bb_trans)
                bb2img_trans_list.append(bb2img_trans)
                flag_flip_list = [False]

                return bbox_list, img_crop_list, img2bb_trans_list, bb2img_trans_list, flag_flip_list
        else:
            bbox_list, img_crop_list, img2bb_trans_list, bb2img_trans_list, flag_flip_list = [], [], [], [], []
            return bbox_list, img_crop_list, img2bb_trans_list, bb2img_trans_list, flag_flip_list

    def extract_twohand(self, img):
        t1 = time.time()

        img_height, img_width, _ = img.shape

        image = cv2.flip(img, 1)
        results = self.mediahand.process(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))

        t2 = time.time()

        bbox_list = []
        if results.multi_hand_landmarks is not None:
            for hand_landmarks, handedness in zip(results.multi_hand_landmarks, results.multi_handedness):
                # Bounding box calculation
                bbox = self.calc_bounding_rect(img_width, img_height, hand_landmarks)
                bbox_list.append(bbox)

        t3 = time.time()
        if len(bbox_list) == 0:
            print("no bounding box")
            output = [], [], [], [], []
        else:
            output = self.create_input(img, bbox_list, img_width)

        t4 = time.time()
        print("mediapipe t : ", t2 - t1)        # 20ms
        print("create input t : ", t4 - t3)     # 4ms
        return output

    def calc_bounding_rect(self, image_width, image_height, landmarks):
        landmark_array = np.empty((0, 2), int)
        for _, landmark in enumerate(landmarks.landmark):
            landmark_x = min(int(landmark.x * image_width), image_width - 1)
            landmark_y = min(int(landmark.y * image_height), image_height - 1)
            landmark_point = [np.array((landmark_x, landmark_y))]
            landmark_array = np.append(landmark_array, landmark_point, axis=0)

        x, y, w, h = cv2.boundingRect(landmark_array)
        # x, y : upper right point
        x = image_width - x

        margin = self.crop_size / 4.0
        x_min = max(0, x - margin * 3)
        y_min = max(0, y - margin)

        if (x_min + self.crop_size) > image_width:
            x_min = image_width - self.crop_size
        if (y_min + self.crop_size) > image_height:
            y_min = image_height - self.crop_size

        bbox = [x_min, y_min, self.crop_size, self.crop_size]

        return bbox

    def calc_bounding_rect_coords(self, image_width, image_height, coords):
        x, y, w, h = cv2.boundingRect(coords[:, :2])
        # x, y : upper right point

        x = image_width - x
        x_c = x + w/2
        y_c = y + h/2

        x_min = max(0, x_c - self.crop_size/2)  # *3)
        y_min = max(0, y_c - self.crop_size/2)

        if (x_min + self.crop_size) > image_width:
            x_min = image_width - self.crop_size
        if (y_min + self.crop_size) > image_height:
            y_min = image_height - self.crop_size

        bbox = [x_min, y_min, self.crop_size, self.crop_size]

        return bbox

    def create_input(self, img, bbox_list, width):
        img_crop_list, img2bb_trans_list, bb2img_trans_list = [], [], []

        if len(bbox_list) == 1:
            if bbox_list[0][0] < (width / 2):
                flag_flip_list = [True]
            else:
                flag_flip_list = [False]

            img_crop, img2bb_trans, bb2img_trans, _, _, = augmentation_real(img, bbox_list[0], flip=flag_flip_list[0])
            # cv2.imshow('crop 0', img_crop / 255.)
            # cv2.waitKey(1)
            img_crop_list.append(img_crop)
            img2bb_trans_list.append(img2bb_trans)
            bb2img_trans_list.append(bb2img_trans)

        else:
            if bbox_list[0][0] < bbox_list[1][0]:
                flag_flip_list = [True, False]      # hand order : left - right
            else:
                flag_flip_list = [False, True]      # hand order : right - left

            for idx, bbox in enumerate(bbox_list):
                flag_flip = flag_flip_list[idx]
                img_crop, img2bb_trans, bb2img_trans, _, _, = augmentation_real(img, bbox, flip=flag_flip)
                # cv2.imshow('crop 0', img_crop / 255.)
                # cv2.waitKey(1)
                img_crop_list.append(img_crop)
                img2bb_trans_list.append(img2bb_trans)
                bb2img_trans_list.append(bb2img_trans)

        return bbox_list, img_crop_list, img2bb_trans_list, bb2img_trans_list, flag_flip_list


def main():
    torch.backends.cudnn.benchmark = True
    tracker = HandTracker()
    cam_intrinsic = None

    for i in range(10):
        color = np.random.randint(255, size=(640, 480, 3), dtype=np.uint8)
        pred_list = tracker.Process(color)
        ### if required uvd format ##

        for all_uvd in pred_list:
            mesh_uvd = copy.deepcopy(all_uvd[:cfg.num_vert])       # (778, 3)
            joint_uvd = copy.deepcopy(all_uvd[cfg.num_vert:])       # (21, 3)
            ### if required xyz format ###
            # all_xyz = uvd2xyz(all_uvd, cam_intrinsic)
            # _visualize(color, joint_uvd)
            print("joint : ", joint_uvd)

    print("test end")



def _get_input(frame):
    ### load image from recorded files ###
    load_filepath = './recorded_files/'

    color = cv2.imread(load_filepath + 'color_%d.png' % frame)
    color = cv2.resize(color, dsize=(256, 256), interpolation=cv2.INTER_CUBIC)

    return color

def _visualize(color, coords_uvd):
    vis = draw_2d_skeleton(color, coords_uvd[cfg.num_vert:])
    vis = cv2.resize(vis, dsize=(416, 416), interpolation=cv2.INTER_CUBIC)
    color = cv2.resize(color, dsize=(416, 416), interpolation=cv2.INTER_CUBIC)
    cv2.imshow("vis", vis)
    cv2.imshow("img", color)
    cv2.waitKey(50)

def uvd2xyz(uvd, K):
    fx, fy, fu, fv = K[0, 0], K[0, 0], K[0, 2], K[1, 2]
    xyz = np.zeros_like(uvd, np.float32)
    xyz[:, 0] = (uvd[:, 0] - fu) * uvd[:, 2] / fx
    xyz[:, 1] = (uvd[:, 1] - fv) * uvd[:, 2] / fy
    xyz[:, 2] = uvd[:, 2]
    return xyz

def xyz2uvd(xyz, K):
    fx, fy, fu, fv = K[0, 0], K[0, 0], K[0, 2], K[1, 2]
    uvd = np.zeros_like(xyz, np.float32)
    uvd[:, 0] = (xyz[:, 0] * fx / xyz[:, 2] + fu)
    uvd[:, 1] = (xyz[:, 1] * fy / xyz[:, 2] + fv)
    uvd[:, 2] = xyz[:, 2]
    return uvd

if __name__ == '__main__':
    main()



